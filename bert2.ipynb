{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8b90499cd129477e90b07bbb797a2f5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_20e61396a18545d685d0fb0526489ddf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d56f5de150e7443bbb4c0e892c71fbec",
              "IPY_MODEL_2415c9685f644c479e451a71cdc8bdf5"
            ]
          }
        },
        "20e61396a18545d685d0fb0526489ddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d56f5de150e7443bbb4c0e892c71fbec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7c2a863e91014db28c6e1afa0afce86e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_46def864749c4d00bf5b62a19b7becc9"
          }
        },
        "2415c9685f644c479e451a71cdc8bdf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e073873cf167423ebf08d2f72dac8d39",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:04&lt;00:00, 43.7kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_615ceabc30a14e06a2c1093b3ba3d3ef"
          }
        },
        "7c2a863e91014db28c6e1afa0afce86e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "46def864749c4d00bf5b62a19b7becc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e073873cf167423ebf08d2f72dac8d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "615ceabc30a14e06a2c1093b3ba3d3ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bcd3a2e6b39b4774a7da14ea51de564e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_da7a4d861af144a097b6469bafdd9dd2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2a9b0e825aaf45428d5931b31ada16b9",
              "IPY_MODEL_65802953cb724cacb0396e0cb1c74f86"
            ]
          }
        },
        "da7a4d861af144a097b6469bafdd9dd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a9b0e825aaf45428d5931b31ada16b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f400de147d60495292312bc3bc55f714",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 29,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 29,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0f382d50053416a8d79518a5b9435d3"
          }
        },
        "65802953cb724cacb0396e0cb1c74f86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_90955ceda6c74a20b354fa7d5bc46602",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29.0/29.0 [00:01&lt;00:00, 24.4B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e3970d54ad8c4919a5ae2e08e90858e5"
          }
        },
        "f400de147d60495292312bc3bc55f714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0f382d50053416a8d79518a5b9435d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "90955ceda6c74a20b354fa7d5bc46602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e3970d54ad8c4919a5ae2e08e90858e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c83a6734af114555a6d8912ad51c5011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7c98b3b470884a588e6b969d0720a373",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_83b1c3b583744d739ce7efa8e01599cc",
              "IPY_MODEL_e828583d3d9946b2adc1bac7780adf14"
            ]
          }
        },
        "7c98b3b470884a588e6b969d0720a373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83b1c3b583744d739ce7efa8e01599cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_93a4dbd2cfa145208385dc1ff03f1933",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435797,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435797,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_19c71dc2cc444194b327eb56b1df6026"
          }
        },
        "e828583d3d9946b2adc1bac7780adf14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_df2b53dfc93d46589edf815c5ad6db7e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436k/436k [00:00&lt;00:00, 564kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_23027e8449aa42e1b89626bf379de048"
          }
        },
        "93a4dbd2cfa145208385dc1ff03f1933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "19c71dc2cc444194b327eb56b1df6026": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df2b53dfc93d46589edf815c5ad6db7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "23027e8449aa42e1b89626bf379de048": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "29e6f241338043ce85bdc74e491a4006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3d8d5ca4f060465f93d53b63f1ca4a3e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_221ebb91e4624235aa098d97cdd5862b",
              "IPY_MODEL_8f38e2101bc040caa60e7033b53ecb4d"
            ]
          }
        },
        "3d8d5ca4f060465f93d53b63f1ca4a3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "221ebb91e4624235aa098d97cdd5862b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f6b3cb7a4feb4673a5b5d9562270ab36",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b8a51046e80b4dfb802739646fb0b93f"
          }
        },
        "8f38e2101bc040caa60e7033b53ecb4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_28c58e26c3f544e787020b9fa184da95",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 718B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41fb9e0da6d84713bc15c57973ac22a5"
          }
        },
        "f6b3cb7a4feb4673a5b5d9562270ab36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b8a51046e80b4dfb802739646fb0b93f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28c58e26c3f544e787020b9fa184da95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41fb9e0da6d84713bc15c57973ac22a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98e12af93cbb49789292da2ec237c82c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c8df782918ec4748b380e30d1bb3060c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b46375767f184d418cdc90b19d826fe6",
              "IPY_MODEL_4874dd7251a54585930270a04c149914"
            ]
          }
        },
        "c8df782918ec4748b380e30d1bb3060c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b46375767f184d418cdc90b19d826fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9b84de8e59fd4d9c9a873b90f296a1ec",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bb5da4cfcd2741c9bb8722a17dbcf192"
          }
        },
        "4874dd7251a54585930270a04c149914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ce21ab4302f84d2dae11e8f96ad4b612",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:08&lt;00:00, 52.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e62e9d4ac314c948882ccec11003438"
          }
        },
        "9b84de8e59fd4d9c9a873b90f296a1ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bb5da4cfcd2741c9bb8722a17dbcf192": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce21ab4302f84d2dae11e8f96ad4b612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e62e9d4ac314c948882ccec11003438": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnantShankhdhar/Flipkart_Category_Prediction/blob/master/bert2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBAHeUTH-9f9"
      },
      "source": [
        "# Flipkart Product Category Classification "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTKnw3NS_X5h"
      },
      "source": [
        "## Loading Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNtRQLtmI2Yw",
        "outputId": "516f0e95-b004-4616-a728-aa2dfd71cb02"
      },
      "source": [
        "!pip3 install transformers\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import TensorDataset,WeightedRandomSampler\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "!pip3 install sentencepiece\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.2MB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 19.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 870kB 36.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=63f7d58f1e25c5d12df81727615f9094e00c5b2d15eb2987d91c85b315512cc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 5.9MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ_kfhTOJIpZ"
      },
      "source": [
        "! cd drive/MyDrive/MIDAS_task"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggj3FhAWU8gP",
        "outputId": "79b7ae95-bb8d-4554-efb2-d95789729e42"
      },
      "source": [
        "! pwd\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZLP2udkyk8T",
        "outputId": "085eb343-ee6d-4028-e05e-df9b34e545f9"
      },
      "source": [
        "% cd drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS0rFN3aywVC",
        "outputId": "bc9486da-b5b6-49aa-8e72-bd63ba1370c5"
      },
      "source": [
        "% cd MyDrive/MIDAS_task"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/MIDAS_task\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2Q3G6JzAsqq"
      },
      "source": [
        "## DataSet Exploration \n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "utZLnu4JzOvr",
        "outputId": "a5300124-ad4d-491d-c30f-5744f977c047"
      },
      "source": [
        "df = pd.read_csv('outfinal1.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>product_category_tree</th>\n",
              "      <th>description</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>clothing</td>\n",
              "      <td>key feature alisha solid women cycling short c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>furniture</td>\n",
              "      <td>fabhomedecor fabric double sofa bed finish col...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>footwear</td>\n",
              "      <td>key feature aw belly sandal wedge heel casuals...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>clothing</td>\n",
              "      <td>key feature alisha solid women cycling short c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pet supplies</td>\n",
              "      <td>specification sicon purpose arnica shampoo ml ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  product_category_tree  ... label\n",
              "0              clothing  ...     0\n",
              "1             furniture  ...     1\n",
              "2              footwear  ...     2\n",
              "3              clothing  ...     0\n",
              "4          pet supplies  ...     3\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "hJkXkioCzTQQ",
        "outputId": "347b0316-a4c4-46b2-8ad7-6f968da85f8f"
      },
      "source": [
        "\n",
        "sns.countplot(df.label)\n",
        "\n",
        "plt.xlabel('label');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlYHoZhwBrjB"
      },
      "source": [
        "## Loading BERT \n",
        "\n",
        "So for the 3rd model, we are using BERT and fine tunning it for our classification task. <br> \n",
        "BERT has shown to deliver best in class results for a variety of Natural Language Processing tasks, it uses the Encoder from the Transformer Architecture. Transformer changed the NLP domain, by giving best in class results in the task for Language Translation. <br>\n",
        "It is based on the attention mechanim which mimics the selective attention mechanism which are mind uses to find the important terms in a given sentence, thus it is surely supposed to do a better task in generalization for our category classification task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181,
          "referenced_widgets": [
            "8b90499cd129477e90b07bbb797a2f5b",
            "20e61396a18545d685d0fb0526489ddf",
            "d56f5de150e7443bbb4c0e892c71fbec",
            "2415c9685f644c479e451a71cdc8bdf5",
            "7c2a863e91014db28c6e1afa0afce86e",
            "46def864749c4d00bf5b62a19b7becc9",
            "e073873cf167423ebf08d2f72dac8d39",
            "615ceabc30a14e06a2c1093b3ba3d3ef",
            "bcd3a2e6b39b4774a7da14ea51de564e",
            "da7a4d861af144a097b6469bafdd9dd2",
            "2a9b0e825aaf45428d5931b31ada16b9",
            "65802953cb724cacb0396e0cb1c74f86",
            "f400de147d60495292312bc3bc55f714",
            "a0f382d50053416a8d79518a5b9435d3",
            "90955ceda6c74a20b354fa7d5bc46602",
            "e3970d54ad8c4919a5ae2e08e90858e5",
            "c83a6734af114555a6d8912ad51c5011",
            "7c98b3b470884a588e6b969d0720a373",
            "83b1c3b583744d739ce7efa8e01599cc",
            "e828583d3d9946b2adc1bac7780adf14",
            "93a4dbd2cfa145208385dc1ff03f1933",
            "19c71dc2cc444194b327eb56b1df6026",
            "df2b53dfc93d46589edf815c5ad6db7e",
            "23027e8449aa42e1b89626bf379de048"
          ]
        },
        "id": "2O6gxO7Jzs2o",
        "outputId": "89f2e815-86fe-485e-fb2d-6edd717a1052"
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)#tokenizer for bert model\n",
        "tokenizer.sep_token, tokenizer.sep_token_id#tokenizer for end of sentence\n",
        "tokenizer.cls_token, tokenizer.cls_token_id#tokenizer for start of sentence \n",
        "tokenizer.pad_token, tokenizer.pad_token_id#padding token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b90499cd129477e90b07bbb797a2f5b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcd3a2e6b39b4774a7da14ea51de564e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_wâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c83a6734af114555a6d8912ad51c5011",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[PAD]', 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOXIqEGz_p0V",
        "outputId": "b91cecac-1403-41e8-b52b-29a0224933ca"
      },
      "source": [
        "tokenizer.unk_token, tokenizer.unk_token_id#tokenizer for unknown words\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[UNK]', 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt8dhd0QJ8PI"
      },
      "source": [
        "As bert works on fixed length sentences we need to find a max_len to which we wish to tokenize a sentence we find that by plotting a curve of frequency of sentences vs number of tokens "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl_6UGaTAEyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d6d2ea-2f86-4e50-bfd4-93896f462d11"
      },
      "source": [
        "token_lens = []\n",
        "for txt in df.description:\n",
        "  tokens = tokenizer.encode(txt, max_length=512)\n",
        "  token_lens.append(len(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSB74K9DAjsu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "169a702e-bf5d-4051-d827-283f0baea486"
      },
      "source": [
        "sns.distplot(token_lens)\n",
        "plt.xlim([0, 450]);\n",
        "plt.xlabel('Token count');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w_mEehRKPcL"
      },
      "source": [
        "As we see that most sentences have token count less than 300 we chose 300 as the max length for sentence for our bert model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQiKEPNsAxK-"
      },
      "source": [
        "MAX_LEN = 300\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybOV7nLyKeGZ"
      },
      "source": [
        "Now we need to load the dataset. We are using pytorch so we create the dataset class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-MAK0GTEQh3"
      },
      "source": [
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "class FlipkartDataset(Dataset):\n",
        "  def __init__(self, descriptions, labels, tokenizer, max_len):\n",
        "    self.descriptions = descriptions\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  def __len__(self):\n",
        "    return len(self.descriptions)\n",
        "  def __getitem__(self, item):\n",
        "    description = str(self.descriptions[item])\n",
        "    label = self.labels[item]\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      description,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "    return {\n",
        "      'description_text': description,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'labels': torch.tensor(label, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wluCeZdhKqXJ"
      },
      "source": [
        "As we have less data points, for splitting the data I am using 18000 examples for train , 1000 for test and 1000 for validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAPP5IrepT1V"
      },
      "source": [
        "\n",
        "df_train, df_test = train_test_split(\n",
        "  df,\n",
        "  test_size=0.1,\n",
        "  random_state=RANDOM_SEED\n",
        ")\n",
        "df_val, df_test = train_test_split(\n",
        "  df_test,\n",
        "  test_size=0.5,\n",
        "  random_state=RANDOM_SEED\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqMWkrpKJ0Pk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "150d722d-5430-4d61-e78c-34e0bfbf1ead"
      },
      "source": [
        "print(df_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        product_category_tree  ... label\n",
            "5848                jewellery  ...    16\n",
            "9742                 clothing  ...     0\n",
            "15779                footwear  ...     2\n",
            "13249               jewellery  ...    16\n",
            "4489                jewellery  ...    16\n",
            "...                       ...  ...   ...\n",
            "16205               computers  ...    20\n",
            "6056   toys & school supplies  ...    15\n",
            "16844         home furnishing  ...    11\n",
            "16827               jewellery  ...    16\n",
            "3721               automotive  ...     9\n",
            "\n",
            "[1000 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-waf7YU0LZrm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkn026QrLZ2U"
      },
      "source": [
        "## Creating data loader and Addressing Class imbalance\n",
        "Our dataset has a huge class imbalance as seen in the graph above. So in order to address that we will be using weighted random sampler as the sampler in our dataloader for training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO3jO8Jp4atz"
      },
      "source": [
        "\n",
        "labels_unique, counts = np.unique(df_train['label'],return_counts = True)\n",
        "#print('Unique labels : {}'.format(labels_unique))\n",
        "class_weights =  [sum(counts)/c for c in counts]\n",
        "example_weights = [class_weights[e] for e in df_train['label']]\n",
        "sampler  =  WeightedRandomSampler(example_weights,len(df_train['label']))\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size,sampler):\n",
        "  ds = FlipkartDataset(\n",
        "    descriptions=df.description.to_numpy(),\n",
        "    labels=df.label.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    sampler = sampler,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWoZi60bL9dO"
      },
      "source": [
        "Creating train test and val data loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ual_lLmPmviM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afea3b52-5c10-40d5-ce7f-3f8ffc68478d"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE,sampler)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE,None)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE,None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8QYK34ipFc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f7d32c-2119-401e-de20-1c6da4829aa4"
      },
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()\n",
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['labels'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 300])\n",
            "torch.Size([16, 300])\n",
            "torch.Size([16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "789By2ij1ENb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db7f47c7-012d-421c-f57c-93ac61d394f3"
      },
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17998, 3), (1000, 3), (1000, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbJ_HE1MEck"
      },
      "source": [
        "#Loading the bert model from transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0DThSi93cL0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "29e6f241338043ce85bdc74e491a4006",
            "3d8d5ca4f060465f93d53b63f1ca4a3e",
            "221ebb91e4624235aa098d97cdd5862b",
            "8f38e2101bc040caa60e7033b53ecb4d",
            "f6b3cb7a4feb4673a5b5d9562270ab36",
            "b8a51046e80b4dfb802739646fb0b93f",
            "28c58e26c3f544e787020b9fa184da95",
            "41fb9e0da6d84713bc15c57973ac22a5",
            "98e12af93cbb49789292da2ec237c82c",
            "c8df782918ec4748b380e30d1bb3060c",
            "b46375767f184d418cdc90b19d826fe6",
            "4874dd7251a54585930270a04c149914",
            "9b84de8e59fd4d9c9a873b90f296a1ec",
            "bb5da4cfcd2741c9bb8722a17dbcf192",
            "ce21ab4302f84d2dae11e8f96ad4b612",
            "7e62e9d4ac314c948882ccec11003438"
          ]
        },
        "outputId": "711951bc-a94a-423b-8d6b-5fff143bdf8a"
      },
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29e6f241338043ce85bdc74e491a4006",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98e12af93cbb49789292da2ec237c82c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12uWK7OkMcjN"
      },
      "source": [
        "##Creating the classifier \n",
        "We will create the classifier by adding a final classification layer ahead of the BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umKVy-UeGJiU"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, n_classes):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      return_dict = False\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FcSVpucGWZD"
      },
      "source": [
        "model = Classifier(28)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRqySEX3M7ey"
      },
      "source": [
        "Getting the inputs ready for training by moving them to the gpu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S6J_-JqE3uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d02a4f5-d66c-4b33-d1d7-134677ff4242"
      },
      "source": [
        "model = model.to(device)\n",
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length\n",
        "print(input_ids.type)\n",
        "print(attention_mask.type)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 300])\n",
            "torch.Size([16, 300])\n",
            "<built-in method type of Tensor object at 0x7fa726816370>\n",
            "<built-in method type of Tensor object at 0x7fa726816780>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsqMwEgvNXge"
      },
      "source": [
        "##Creating the training function\n",
        "We will be training for 10 epochs using AdamW optimizer with learning rate 2e-5 with a batch size of 16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqkQLQjSOau5"
      },
      "source": [
        "EPOCHS = 10\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    labels = d[\"labels\"].to(device)\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "    )\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    correct_predictions += torch.sum(preds == labels)\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuJo5xQTN7vb"
      },
      "source": [
        "##Creating the evaluation function\n",
        "We make the evaluation function to run in training loop to calculate validation accuracy . The best validation accuracy model will get saved"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNzcFxaJOkec"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      labels = d[\"labels\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      correct_predictions += torch.sum(preds == labels)\n",
        "      losses.append(loss.item())\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo5Cx4tQOSEN"
      },
      "source": [
        "##Running the train and test function for 10 epochs to train the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kflp1SLjTUGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e312aee-433d-483c-e7c6-e609da18a086"
      },
      "source": [
        "\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'bert_final_model.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5834804419279098 accuracy 0.8562618068674298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.3307683971444411 accuracy 0.915\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.13125978338283797 accuracy 0.9672185798422047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.21615992489803051 accuracy 0.9490000000000001\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.07515130533381469 accuracy 0.9817757528614292\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.18097699231593795 accuracy 0.962\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.053350516048880914 accuracy 0.986998555395044\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.18989436804420418 accuracy 0.964\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.041686070690707615 accuracy 0.9892210245582843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.20529958301100007 accuracy 0.966\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.03539543747744109 accuracy 0.9920546727414158\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.20580075000688094 accuracy 0.966\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.02647310470549079 accuracy 0.992999222135793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.21829908507523157 accuracy 0.963\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.021048197588482354 accuracy 0.9947216357373042\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.21267765181417214 accuracy 0.969\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.017972675382987493 accuracy 0.9951661295699523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.21656144825193757 accuracy 0.966\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.017007812530940605 accuracy 0.9951105678408713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.22196882488118255 accuracy 0.966\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W8R4f6QOb33"
      },
      "source": [
        "##Loading the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og3QRmFnBRYr",
        "outputId": "8303a9a5-fd04-4288-d099-33016b400836"
      },
      "source": [
        "model.load_state_dict(torch.load('bert_final_model.bin'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00paLKB8Of_i"
      },
      "source": [
        "##Testing the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Dh-3eq77qPK",
        "outputId": "27f06cc9-868a-4eea-d19d-a24e34bc36fa"
      },
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmkK7XJO7zWO",
        "outputId": "d2b84b89-bb3e-4010-f506-b8768acefc69"
      },
      "source": [
        "test_acc.item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.973"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIuaXmfZOp5u"
      },
      "source": [
        "##Function to predict the class of the text\n",
        "Now that we have tested we use predictions from the test set for other evaluations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTU0IPc2BplU"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  descriptions = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      description_text = d[\"description_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      labels = d[\"labels\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      descriptions.extend(description_text)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(outputs)\n",
        "      real_values.extend(labels)\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return descriptions, predictions, prediction_probs, real_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie1VKjXNHTr-",
        "outputId": "ad0c2ae0-d813-4d00-f482-b21c64e3a7c1"
      },
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwPmcIP0O2yF"
      },
      "source": [
        "##Measuring Accuracy accross different parameters\n",
        "We use precision, recall and F1 score to check the performance of our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff93lP3CHX1E",
        "outputId": "0d80744d-b783-4f5f-863f-0203b5aa418c"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "class_names = ['clothing', 'furniture', 'footwear', 'pet supplies', 'pens & stationery', 'sports & fitness', 'beauty and personal care', 'bags, wallets & belts', 'home decor & festive needs', 'automotive', 'tools & hardware', 'home furnishing', 'baby care', 'mobiles & accessories', 'watches', 'toys & school supplies', 'jewellery', 'sunglasses', 'kitchen & dining', 'home & kitchen', 'computers', 'cameras & accessories', 'health & personal care appliances', 'gaming', 'home improvement', 'home entertainment', 'others']\n",
        "print(len(class_names))\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27\n",
            "                                   precision    recall  f1-score   support\n",
            "\n",
            "                         clothing       0.99      0.98      0.98       297\n",
            "                        furniture       1.00      1.00      1.00        15\n",
            "                         footwear       0.97      1.00      0.98        59\n",
            "                     pet supplies       1.00      1.00      1.00         1\n",
            "                pens & stationery       1.00      1.00      1.00        12\n",
            "                 sports & fitness       1.00      0.92      0.96        12\n",
            "         beauty and personal care       0.97      1.00      0.99        36\n",
            "            bags, wallets & belts       0.94      0.94      0.94        17\n",
            "       home decor & festive needs       0.97      0.94      0.96        36\n",
            "                       automotive       0.94      1.00      0.97        44\n",
            "                 tools & hardware       1.00      1.00      1.00        12\n",
            "                  home furnishing       1.00      1.00      1.00        37\n",
            "                        baby care       0.96      0.93      0.94        27\n",
            "            mobiles & accessories       0.98      0.97      0.97        59\n",
            "                          watches       1.00      1.00      1.00        26\n",
            "           toys & school supplies       0.94      0.94      0.94        18\n",
            "                        jewellery       1.00      0.99      0.99       187\n",
            "                       sunglasses       0.67      1.00      0.80         2\n",
            "                 kitchen & dining       0.95      1.00      0.98        40\n",
            "                   home & kitchen       1.00      1.00      1.00         1\n",
            "                        computers       0.94      0.94      0.94        36\n",
            "            cameras & accessories       1.00      1.00      1.00         4\n",
            "health & personal care appliances       1.00      1.00      1.00         2\n",
            "                           gaming       1.00      1.00      1.00         1\n",
            "                 home improvement       1.00      1.00      1.00         5\n",
            "               home entertainment       1.00      1.00      1.00         2\n",
            "                           others       0.30      0.25      0.27        12\n",
            "\n",
            "                         accuracy                           0.97      1000\n",
            "                        macro avg       0.95      0.96      0.95      1000\n",
            "                     weighted avg       0.97      0.97      0.97      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G20_zeavLZUX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}